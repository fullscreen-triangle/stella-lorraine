\section{Categorical Entropy}
\label{sec:categorical}

\subsection{Categories as Distinguishable States}

From the triple equivalence (Theorem~\ref{thm:triple_equivalence}), an oscillating system traverses $M$ distinguishable states per period. Each state is a \textit{category}—a region of phase space that the system occupies at some point during its evolution.

Consider a system with $M$ categorical dimensions, each capable of distinguishing $n$ states. The total number of distinguishable configurations is:
\begin{equation}
W = n^M
\end{equation}

This follows from the independence of categorical dimensions: each of the $M$ dimensions can be in any of its $n$ states, giving $n \times n \times \cdots \times n = n^M$ total configurations.

\subsection{Derivation of Categorical Entropy}

Following Boltzmann's fundamental postulate~\cite{boltzmann1877}, entropy is proportional to the logarithm of the number of accessible microstates:
\begin{equation}
S = k_B \ln W = k_B \ln(n^M) = k_B M \ln n
\end{equation}

This yields the categorical entropy formula:
\begin{equation}
\boxed{S_{\text{cat}} = k_B M \ln n}
\label{eq:categorical_entropy}
\end{equation}

\textbf{Physical interpretation:}
\begin{itemize}
\item $M$ is the number of categorical dimensions (degrees of freedom)
\item $n$ is the number of distinguishable states per dimension
\item $\ln n$ is the information content per categorical dimension
\item $k_B$ converts information units (nats) to thermodynamic units (J/K)
\end{itemize}

\begin{figure}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/fig_entropy_derivations.png}
\caption{\textbf{Triple Entropy Equivalence: Three Perspectives on the Same Structure.} 
\textbf{Left Panel - Categorical Entropy:} Five categories (C1-C5, green boxes) each containing $n = 4$ states (green dots). Total configurations: $W = n^M = 4^5 = 1024$. Categorical entropy: $S_{\text{cat}} = k_B \ln W = k_B M \ln n$ (yellow box). This perspective counts distinguishable states.
\textbf{Center Panel - Oscillatory Entropy:} Four oscillators with amplitudes $A_1, A_2, A_3, A_4$ (blue ellipses with red arrows showing phase). Each oscillator occupies phase space volume $\Gamma_i = \pi m \omega A_i^2$. Ratio to ground state: $\Gamma_i/\Gamma_0 = (A_i/A_0)^2$. Oscillatory entropy: $S_{\text{osc}} = k_B \sum_i \ln(\Gamma_i/\Gamma_0) = k_B \sum_i \ln(A_i/A_0)^2$ (cyan box). This perspective measures phase space volume.
\textbf{Right Panel - Partition Entropy:} Four partitions (pink trapezoids with black arrows) showing temporal decomposition. Selectivity values: $s_1 = 0.5$, $s_2 = 0.25$, $s_3 = 0.34$, $s_4 = 0.1$. Selectivity defined as $s_a = 1/n_a$ (inverse depth). Information per aperture: $I_a = \ln(1/s_a) = \ln(n_a)$. Partition entropy: $S_{\text{part}} = k_B \sum_a \ln(1/s_a)$ (pink box). This perspective measures temporal resolution.
\textbf{Bottom - Equivalence Condition:} Yellow box states fundamental result: $S_{\text{cat}} = S_{\text{osc}} = S_{\text{part}}$ when $n = (A/A_0)^2 = 1/s$. This condition ensures all three perspectives yield identical entropy values. The triple equivalence is not approximate but exact under this correspondence.}
\label{fig:entropy_derivations}
\end{figure}

\subsection{Relation to Classical Statistical Mechanics}

The classical Boltzmann entropy is:
\begin{equation}
S_{\text{Boltzmann}} = k_B \ln \Omega
\end{equation}
where $\Omega$ is the total number of accessible microstates.

When the phase space factorises into $M$ independent dimensions with $n$ states each, we have $\Omega = n^M$, and:
\begin{equation}
S_{\text{Boltzmann}} = k_B \ln(n^M) = k_B M \ln n = S_{\text{cat}}
\end{equation}

The categorical form is more fundamental because it explicitly separates:
\begin{itemize}
\item The \textit{number} of distinctions ($M$) — how many degrees of freedom there are
\item The \textit{depth} of each distinction ($\ln n$) — how finely each degree of freedom is resolved
\end{itemize}

This separation is not merely notational. It becomes essential when $M$ varies dynamically (as in systems with temperature-dependent degrees of freedom) or when different dimensions have different resolutions. The categorical form makes explicit that entropy has two independent sources: the dimensionality of the phase space and the resolution within each dimension.

\subsection{Information-Theoretic Foundation}

The categorical entropy has direct information-theoretic meaning. Consider $M$ categorical dimensions, each with $n$ equally probable states. The Shannon entropy~\cite{shannon1948} of this system is:
\begin{equation}
H = -\sum_{i=1}^{n^M} p_i \ln p_i
\end{equation}

For uniform probability $p_i = 1/n^M$:
\begin{equation}
H = -n^M \cdot \frac{1}{n^M} \ln\frac{1}{n^M} = \ln(n^M) = M \ln n
\end{equation}

Thus:
\begin{equation}
S_{\text{cat}} = k_B H
\end{equation}

The categorical entropy is Boltzmann's constant times the Shannon information content. This establishes a precise correspondence:
\begin{center}
\begin{tabular}{lcl}
Thermodynamic entropy & $\longleftrightarrow$ & Information content \\
$k_B$ (energy/temperature) & $\longleftrightarrow$ & 1 nat \\
Temperature & $\longleftrightarrow$ & Energy cost per bit \\
\end{tabular}
\end{center}

\subsection{Extensivity and Additivity}

Categorical entropy is extensive. For two independent subsystems with $(M_1, n_1)$ and $(M_2, n_2)$:
\begin{equation}
S_{\text{total}} = k_B M_1 \ln n_1 + k_B M_2 \ln n_2
\end{equation}

If the subsystems have identical categorical structures ($n_1 = n_2 = n$):
\begin{equation}
S_{\text{total}} = k_B (M_1 + M_2) \ln n = k_B M_{\text{total}} \ln n
\end{equation}

Categorical dimensions are additive. This is the microscopic origin of entropy extensivity: combining systems increases the total number of categorical dimensions while preserving the resolution per dimension.

For $N$ identical subsystems, each with $M_0$ dimensions:
\begin{equation}
S_{\text{total}} = N k_B M_0 \ln n = k_B M_{\text{total}} \ln n
\end{equation}
where $M_{\text{total}} = N M_0$. This recovers the familiar extensive scaling $S \propto N$.

\subsection{Categorical Temperature}

Temperature emerges from the thermodynamic relation:
\begin{equation}
\frac{1}{T} = \left(\frac{\partial S}{\partial U}\right)_{V,N}
\end{equation}

Substituting $S = k_B M \ln n$:
\begin{equation}
\frac{1}{T} = k_B \ln n \cdot \frac{\partial M}{\partial U}
\end{equation}

The key insight is that energy determines the number of accessible categorical dimensions. For a quantum oscillator, each quantum $\hbar\omega$ of energy activates one additional categorical dimension:
\begin{equation}
M = \frac{U}{\hbar\omega}
\quad \Rightarrow \quad
\frac{\partial M}{\partial U} = \frac{1}{\hbar\omega}
\end{equation}

Thus:
\begin{equation}
\frac{1}{T} = \frac{k_B \ln n}{\hbar\omega}
\quad \Rightarrow \quad
T = \frac{\hbar\omega}{k_B \ln n}
\end{equation}

For the natural choice $n = e$ (one nat of information per categorical dimension), we have $\ln n = 1$ and:
\begin{equation}
\boxed{T = \frac{\hbar\omega}{k_B}}
\label{eq:categorical_temperature}
\end{equation}

This is precisely the quantum mechanical temperature for a single oscillator mode. The categorical perspective reveals that temperature measures the energy cost per categorical dimension.

Alternatively, solving for the number of active dimensions:
\begin{equation}
M_{\text{active}} = \frac{U}{k_B T}
\end{equation}

At temperature $T$, the system has $U/(k_B T)$ thermally active categorical dimensions. This provides a direct physical interpretation: temperature sets the scale for how many dimensions are energetically accessible.

\subsection{Dynamic Categories and Entropy Production}

The number of accessible categorical dimensions can change as the system evolves. The rate of entropy production is:
\begin{equation}
\frac{dS}{dt} = k_B \ln n \cdot \frac{dM}{dt}
\end{equation}

This has a striking interpretation: \textit{entropy increases at a rate proportional to the categorical actualisation rate}. Rapid exploration of new categorical dimensions corresponds to rapid entropy increase.

At equilibrium, when all accessible dimensions have been uniformly explored, the system reaches a steady state where $\langle dM/dt \rangle = 0$ (averaged over the period) and entropy production cease. The system has fully actualised its categorical structure.

For a system approaching equilibrium from a constrained initial state:
\begin{equation}
S(t) - S(0) = k_B \ln n \cdot [M(t) - M(0)]
\end{equation}

The increase in entropy equals the number of newly accessed categorical dimensions multiplied by the information content per dimension. This provides a microscopic picture of the second law: isolated systems spontaneously explore previously inaccessible regions of categorical space.

\begin{figure*}[htbp]
\centering
\includegraphics[width=\textwidth]{figures/topology_categories_panel.png}
\caption{\textbf{Topology of Categorical Spaces: Fundamental Structures and Dynamics.} 
\textbf{(A)} Partial order (completion precedence) showing Hasse diagram of categorical completion relationships. Seven teal nodes connected by edges indicating precedence: bottom node must complete before middle layer (3 nodes), which must complete before top layer (3 nodes). Diamond structure represents meet-semilattice where multiple paths converge to common completion states. 
\textbf{(B)} Tri-dimensional S-space showing three orthogonal axes: $S_k$ (knowledge, blue), $S_t$ (time, green), $S_e$ (evolution/entropy, red). Yellow point indicates example state at coordinates $(S_k, S_t, S_e)$. This 3D space compresses arbitrary high-dimensional phase spaces into universal categorical coordinates. All physical systems map to trajectories in this space. 
\textbf{(C)} $3^k$ branching structure showing complete ternary tree to depth 3. Root node $C$ (dark teal) branches to three children (blue), each branching to three grandchildren (green), each branching to three great-grandchildren (red). Total nodes: $1 + 3 + 9 + 27 = 40$. Color gradient (teal $\to$ blue $\to$ green $\to$ red) indicates increasing depth. Self-similar structure repeats at all scales. 
\textbf{(D)} Scale ambiguity: identical structure at different levels. Two triangular motifs labeled ``Level $n$'' and ``Level $n+1$'' with isomorphism $\Psi_n$ between them. Both triangles have identical topology—three nodes, three edges—demonstrating that sub-structures are indistinguishable from parent structure. This scale invariance is fundamental to categorical measurement: no preferred resolution scale exists. 
\textbf{(E)} Completion trajectory $\gamma(t)$ expanding over time. Green curve shows fraction completed rising from 0.0 to asymptotic limit 1.0 (red dashed line) over 10 time units. Shaded green region represents completed fraction. Trajectory follows $|\gamma(t)|/|C| \to 1$ where $C$ is complete categorical space. Asymptotic approach (not exponential) indicates power-law slowing as remaining unvisited states become sparse. 
\textbf{(F)} Asymptotic slowing: completion rate $\dot{C}(t) \to 0$. Red curve shows completion rate (derivative of panel E) decaying from initial maximum $\sim$0.30 toward zero over 10 time units. Shaded red region under curve represents total completion. Purple dashed line indicates completion time $T$ (asymptotic limit). Hyperbolic decay $\dot{C}(t) \propto 1/t$ characteristic of categorical exploration—increasingly difficult to find new states as coverage increases. Validates prediction that complete categorical coverage is asymptotically approached but never achieved in finite time.}
\label{fig:categorical_topology}
\end{figure*}

\subsection{Resolution Independence}

A crucial feature of categorical entropy is its independence from arbitrary phase space discretization. Classical statistical mechanics requires dividing phase space into cells of volume $h^{3N}$ (where $h$ is often taken as Planck's constant). The choice of $h$ is somewhat arbitrary in classical theory, leading to ambiguities in absolute entropy.

In the categorical formulation, $M$ is not an arbitrary discretization but the number of physically distinguishable states—determined by the oscillation structure itself. The categories are defined by the dynamics, not imposed externally.

For example, a quantum harmonic oscillator has $M = n_{\text{max}}$ categorical dimensions corresponding to energy levels $0, \hbar\omega, 2\hbar\omega, \ldots, n_{\text{max}}\hbar\omega$. These are not arbitrary bins but rather physically distinct quantum states. The categorical entropy:
\begin{equation}
S = k_B n_{\text{max}} \ln n
\end{equation}
depends only on the number of accessible quantum states, with no arbitrary discretization parameter.

\subsection{Summary}

The categorical perspective yields entropy as:
\begin{equation}
S_{\text{cat}} = k_B M \ln n
\end{equation}

Key features:
\begin{enumerate}
\item \textbf{Structural clarity}: Separates the number of categorical dimensions ($M$) from the information content per dimension ($\ln n$)
\item \textbf{Classical correspondence}: Reduces to Boltzmann entropy $S = k_B \ln \Omega$ when $\Omega = n^M$
\item \textbf{Information-theoretic foundation}: Equals $k_B$ times Shannon entropy, establishing thermodynamics as physical information theory
\item \textbf{Extensivity}: Entropy is additive because categorical dimensions are additive
\item \textbf{Temperature}: Emerges as the energy cost per categorical dimension, $T = \hbar\omega/k_B$ for quantum oscillators
\item \textbf{Resolution independence}: Categories are defined by the dynamics, not by arbitrary phase space discretization
\item \textbf{Dynamic interpretation}: Entropy production measures the rate of categorical actualisation
\end{enumerate}

In the following sections, we derive entropy from the oscillatory perspective (Section~\ref{sec:oscillatory}) and the partition perspective (Section~\ref{sec:partition}), then prove that all three formulations are mathematically equivalent (Section~\ref{sec:enthalpy}).
