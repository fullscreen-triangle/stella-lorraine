%==============================================================================
\section{Entropy Mechanism Through Network Topology}
\label{sec:entropy}
%==============================================================================

The preceding sections established that molecular systems evolve through categorical state space according to phase-lock network topology, independently of kinetic energy, and that temperature emerges as a statistical property of cluster structure rather than determining it. We now address the central question of thermodynamics: what is the mechanism by which entropy increases, and how is the Second Law preserved in the categorical framework? We prove that entropy arises from phase-lock network topology, not from spatial disorder or kinetic energy distribution. The apparent paradox of Maxwell's demon is resolved by recognizing that categorical completion—the process traditionally attributed to the demon's "sorting"—actually increases entropy through network densification. This provides a topological mechanism for the Second Law that does not invoke information-theoretic arguments, measurement costs, or erasure operations. Entropy increase is a consequence of categorical structure evolution, not a constraint imposed by information theory.

\subsection{Topological Origin of Entropy}

We begin by establishing that entropy is fundamentally a property of phase-lock network topology rather than spatial configuration or kinetic energy distribution. This topological perspective reveals that entropy quantifies categorical richness—the number of distinct categorical states compatible with a given network structure—rather than spatial disorder.

\begin{definition}[Network Entropy]
\label{def:network_entropy}
The \textbf{network entropy} of a phase-lock configuration is defined as:
\begin{equation}
S_{\phaselockgraph} = k_B \log \Omega_{\text{PL}}(\phaselockgraph)
\label{eq:network_entropy}
\end{equation}
where $\Omega_{\text{PL}}(\phaselockgraph)$ is the number of categorical states compatible with the phase-lock network topology $\phaselockgraph = (V, E)$, and $k_B$ is Boltzmann's constant. The network entropy quantifies the categorical richness of the system: how many distinct ways the system can organize itself categorically while maintaining the same phase-lock structure.
\end{definition}

\begin{remark}[Contrast with Boltzmann Entropy]
\label{rem:contrast_boltzmann}
The classical Boltzmann entropy $S_B = k_B \log \Omega_{\text{spatial}}$ counts the number of spatial microstates compatible with a macrostate, where microstates are distinguished by particle positions and momenta. Network entropy $S_{\phaselockgraph}$ counts the number of categorical states compatible with a network topology, where categorical states are distinguished by phase-lock relationships and cluster membership. The two entropies are complementary: Boltzmann entropy quantifies spatial disorder, while network entropy quantifies categorical richness. As we prove below, network entropy increases through categorical completion even when Boltzmann entropy appears to decrease through spatial ordering, resolving the Maxwell's demon paradox.
\end{remark}

\begin{proposition}[Entropy and Edge Density]
\label{prop:entropy_edge_density}
Network entropy increases with edge density in the phase-lock network. Specifically, network entropy is proportional to the number of edges:
\begin{equation}
S_{\phaselockgraph} \propto k_B |E(\phaselockgraph)|
\label{eq:entropy_edges}
\end{equation}
where $|E(\phaselockgraph)|$ is the number of edges in the phase-lock network. More edges correspond to more phase-lock constraints, which counterintuitively increase entropy by creating richer categorical structure.
\end{proposition}

\begin{proof}
Each edge $(m_i, m_j) \in E$ in the phase-lock network represents a phase-lock constraint: the phase difference $\Phi_i - \Phi_j$ between molecules $i$ and $j$ must remain within the phase-lock bounds determined by the coupling strength $\kappa_{ij}$ and frequency detuning $|\omega_i - \omega_j|$. Specifically, from the phase-lock condition~\eqref{eq:phase_lock_threshold}, molecules $i$ and $j$ are phase-locked when:
\begin{equation}
|\Phi_i - \Phi_j| < \arcsin\left(\frac{|\omega_i - \omega_j|}{2\kappa_{ij}}\right)
\end{equation}

At first glance, constraints reduce the accessible phase space volume: imposing $n$ constraints on an $N$-dimensional phase space typically reduces the accessible volume by a factor exponential in $n$. However, categorical states are not points in phase space but equivalence classes of phase space regions. Each constraint creates a new equivalence relation by partitioning phase space into regions where the constraint is satisfied versus violated. The number of categorical states—the number of distinct equivalence classes—grows with the number of constraints because each new constraint refines the partition.

More formally, consider a system with $N$ molecules and $|E|$ phase-lock edges. Each edge defines a constraint on the relative phases of two molecules. The categorical state space is the quotient space of the full phase space by the equivalence relation generated by these constraints. The number of categorical states is approximately:
\begin{equation}
\Omega_{\text{PL}}(\phaselockgraph) \propto \exp(c \cdot |E|)
\end{equation}
for some constant $c > 0$ that depends on the phase-lock tolerance (the width of the phase-lock region). This exponential growth arises because each edge creates a binary distinction (phase-locked or not), and $|E|$ binary distinctions generate up to $2^{|E|}$ combinations. In practice, not all combinations are realizable due to transitivity constraints (if $i$ is locked to $j$ and $j$ is locked to $k$, then $i$ must be locked to $k$ with compatible phase), so the growth is exponential but with a reduced base.

Taking the logarithm to obtain entropy:
\begin{equation}
S_{\phaselockgraph} = k_B \log \Omega_{\text{PL}} = k_B \log[\exp(c \cdot |E|)] = k_B c \cdot |E|
\end{equation}

Therefore, network entropy is proportional to edge density: $S_{\phaselockgraph} \propto k_B |E|$. This counterintuitive result—more constraints lead to higher entropy—is the key to resolving Maxwell's demon paradox. Constraints do not reduce entropy by limiting possibilities; rather, they increase entropy by creating categorical structure. The demon's apparent "sorting" operation adds constraints (phase-lock relationships) and thereby increases entropy. \qed
\end{proof}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\textwidth]{figures/panel_navigation.png}
    \caption{\textbf{Categorical Navigation: Spatial Distance Irrelevance.}
    \textbf{(A)} Location accessibility in categorical space: all locations (Local,
    Jupiter Core, Sun Center, Deep Space, Earth Mantle, Moon) show equal accessibility
    ($\sim$1.0) despite vastly different physical distances.
    \textbf{(B)} Physical versus categorical distance comparison: physical distances
    span 14 orders of magnitude ($10^0$ to $10^{14}$ km), while categorical distances
    (S-distance) remain uniformly small ($\sim$1 categorical step), demonstrating complete
    decoupling of spatial and categorical metrics.
    \textbf{(C)} Equal measurement time across all locations ($\sim$0.67 ms mean),
    confirming that categorical access time is independent of physical separation.
    \textbf{(D)} Tackle reach comparison showing full tackle accessibility (blue circle)
    versus limited physical reach (pink circle), illustrating that categorical operations
    can access distant states instantaneously.
    \textbf{(E)} Reachability map in $(S_k, S_e)$ entropy coordinate space: all locations
    occupy distinct positions in categorical space (colored circles) with uniform
    reachability (green background), demonstrating that categorical proximity does not
    correspond to physical proximity.
    \textbf{(F)} Sequential access times: all locations accessible within $\sim$0.4--0.9 ms,
    independent of physical distance, confirming that categorical navigation operates
    outside conventional spacetime constraints.}
    \label{fig:navigation}
\end{figure}

\begin{remark}[Why Constraints Increase Entropy]
\label{rem:constraints_entropy}
The proportionality $S \propto |E|$ is counterintuitive from the perspective of classical statistical mechanics, where constraints typically reduce entropy by restricting accessible phase space. The resolution is that network entropy counts categorical distinctions, not phase space volume. Each constraint creates a new categorical distinction (locked versus unlocked), enriching the categorical structure. An analogy: adding words to a language increases its expressive capacity (entropy) even though each word constrains usage through grammatical rules. Similarly, adding phase-lock edges increases categorical richness even though each edge constrains relative phases. The categorical framework reveals that entropy is fundamentally about richness of structure, not absence of structure.
\end{remark}

\subsection{Entropy Increase Through Network Densification}

Having established that entropy is proportional to network edge density, we now prove that processes traditionally understood as "mixing" or "equilibration" correspond to network densification in categorical space. This provides a topological mechanism for entropy increase that does not depend on spatial disorder or kinetic energy redistribution.

\begin{theorem}[Categorical Mixing Increases Entropy]
\label{thm:mixing_entropy}
When two previously separated gas volumes are allowed to mix, entropy increases due to phase-lock network densification. The entropy of mixing is given by:
\begin{equation}
\Delta S_{\text{mix}} = S_{\phaselockgraph_{\text{mixed}}} - S_{\phaselockgraph_{\text{separated}}} = k_B \log \frac{\Omega_{\text{mixed}}}{\Omega_{\text{separated}}} > 0
\label{eq:mixing_entropy}
\end{equation}
where $\Omega_{\text{mixed}}$ and $\Omega_{\text{separated}}$ are the numbers of categorical states in the mixed and separated configurations, respectively. The entropy increase arises from the formation of new phase-lock edges between molecules that were previously in separate volumes and could not interact.
\end{theorem}

\begin{proof}
Consider two gas volumes A and B initially separated by a partition. We analyze the phase-lock network structure before and after removing the partition.

\textbf{Initial (separated) state:}
Volume A contains $N_A$ molecules forming phase-lock network $\phaselockgraph_A = (V_A, E_A)$, where $V_A$ is the set of molecules in A and $E_A$ is the set of phase-lock edges between A-molecules. Similarly, volume B contains $N_B$ molecules forming network $\phaselockgraph_B = (V_B, E_B)$. Since the volumes are separated, there are no phase-lock edges between A-molecules and B-molecules (molecules in different volumes are too far apart to phase-lock). The combined network is the disjoint union:
\begin{equation}
\phaselockgraph_{\text{separated}} = \phaselockgraph_A \sqcup \phaselockgraph_B
\end{equation}
with total edge count:
\begin{equation}
|E_{\text{separated}}| = |E_A| + |E_B|
\end{equation}

The number of categorical states in the separated configuration is:
\begin{equation}
\Omega_{\text{separated}} = \Omega_A \cdot \Omega_B
\end{equation}
where $\Omega_A$ and $\Omega_B$ are the numbers of categorical states in volumes A and B, respectively. The factorization holds because the two volumes are categorically independent (no phase-lock edges connect them).

\textbf{Mixed state:}
After removing the partition, molecules from A can interact with molecules from B. New phase-lock edges form between A-molecules and B-molecules when they come within phase-lock range. The mixed network is:
\begin{equation}
\phaselockgraph_{\text{mixed}} = (V_A \cup V_B, E_A \cup E_B \cup E_{A \leftrightarrow B})
\end{equation}
where $E_{A \leftrightarrow B}$ is the set of edges connecting A-molecules to B-molecules. The total edge count is:
\begin{equation}
|E_{\text{mixed}}| = |E_A| + |E_B| + |E_{A \leftrightarrow B}|
\end{equation}

The number of new edges $|E_{A \leftrightarrow B}|$ depends on the spatial overlap of the two volumes after mixing. In the fully mixed state (uniform density throughout the combined volume), the expected number of A-B edges is:
\begin{equation}
\langle |E_{A \leftrightarrow B}| \rangle = N_A \cdot N_B \cdot P_{\text{lock}}
\end{equation}
where $P_{\text{lock}}$ is the probability that a randomly selected A-molecule and B-molecule are within phase-lock range. For typical gases at standard conditions with phase-lock range $r_{\text{lock}} \sim 2$-$3$ molecular diameters, $P_{\text{lock}}$ is approximately:
\begin{equation}
P_{\text{lock}} \approx \frac{4\pi r_{\text{lock}}^3}{3V_{\text{total}}} \cdot N_{\text{total}}
\end{equation}
where $V_{\text{total}} = V_A + V_B$ is the total volume and $N_{\text{total}} = N_A + N_B$ is the total number of molecules. For typical molecular densities, this gives $P_{\text{lock}} \sim 0.1$ to $0.5$, meaning that approximately 10-50\% of possible A-B pairs are phase-locked at any instant.

Therefore, the number of new edges is substantial:
\begin{equation}
|E_{A \leftrightarrow B}| \sim 0.1 \cdot N_A \cdot N_B \text{ to } 0.5 \cdot N_A \cdot N_B
\end{equation}

For equal volumes with $N_A = N_B = N/2$, this gives:
\begin{equation}
|E_{A \leftrightarrow B}| \sim 0.025 \cdot N^2 \text{ to } 0.125 \cdot N^2
\end{equation}

This is much larger than the original edge counts $|E_A|, |E_B| \sim N$ (since each molecule is phase-locked to a few neighbors, not to all molecules). Therefore:
\begin{equation}
|E_{\text{mixed}}| = |E_{\text{separated}}| + |E_{A \leftrightarrow B}| \gg |E_{\text{separated}}|
\end{equation}

From Proposition~\ref{prop:entropy_edge_density}, entropy is proportional to edge count:
\begin{equation}
S_{\text{mixed}} \propto k_B |E_{\text{mixed}}| \gg k_B |E_{\text{separated}}| \propto S_{\text{separated}}
\end{equation}

The entropy increase is:
\begin{equation}
\Delta S_{\text{mix}} = S_{\text{mixed}} - S_{\text{separated}} \propto k_B |E_{A \leftrightarrow B}| > 0
\end{equation}

This is positive and substantial, confirming that mixing increases entropy through network densification. The mechanism is topological: mixing creates new categorical relationships (phase-lock edges) that did not exist in the separated state, enriching the categorical structure and increasing entropy. \qed

\begin{corollary}[No Entropy Paradox]
\label{cor:no_entropy_paradox}
The apparent "sorting" in Maxwell's thought experiment does not decrease entropy because sorting is categorical completion, not physical rearrangement. Categorical completion always increases network density by establishing new phase-lock relationships, and increased network density increases entropy. Therefore, the demon's operation increases entropy even though it appears to create spatial order. The paradox arises from confusing spatial entropy (which may decrease during sorting) with total entropy (which always increases due to categorical entropy increase dominating spatial entropy decrease).
\end{corollary}

\begin{proof}
The traditional Maxwell's demon scenario involves a demon that sorts molecules by velocity, allowing fast molecules to accumulate in one compartment and slow molecules in another, apparently creating a temperature difference without expending work. This appears to violate the Second Law by decreasing entropy: the sorted state (with temperature gradient) has lower spatial entropy than the mixed state (uniform temperature).

However, from the categorical perspective, the demon's operation is not velocity sorting but categorical completion. As proven in Theorem~\ref{thm:information_free} and Corollary~\ref{cor:demon_actual}, the demon completes categorical states by following phase-lock pathways, which makes adjacent states accessible and initiates cascades of completions (Theorem~\ref{thm:categorical_cascade}).

Each categorical completion establishes new phase-lock relationships. When the demon "observes" a molecule and "decides" to open the door, it is actually completing a categorical state $C_i$, which makes phase-lock adjacent states $\accessible(C_i)$ available. This completion adds edges to the effective phase-lock network:
\begin{equation}
|E(\gamma(t + \delta t))| > |E(\gamma(t))|
\end{equation}
where $\gamma(t)$ is the set of completed states at time $t$.

From Proposition~\ref{prop:entropy_edge_density}, this edge addition increases entropy:
\begin{equation}
\Delta S_{\text{categorical}} = k_B \Delta |E| > 0
\end{equation}

The total entropy change is:
\begin{equation}
\Delta S_{\text{total}} = \Delta S_{\text{spatial}} + \Delta S_{\text{categorical}}
\end{equation}

The spatial entropy may decrease ($\Delta S_{\text{spatial}} < 0$) due to the apparent ordering of molecules into hot and cold compartments. However, the categorical entropy increase ($\Delta S_{\text{categorical}} > 0$) dominates because the number of new phase-lock edges created by categorical completion is much larger than the reduction in spatial microstates. Specifically, sorting $N$ molecules into two compartments reduces spatial entropy by approximately $k_B N \log 2$ (the entropy of mixing for equal volumes). But categorical completion creates $\sim N^2$ new phase-lock edges (since each molecule can phase-lock to many others), increasing entropy by $\sim k_B N^2$. For large $N$, the categorical increase dominates:
\begin{equation}
\Delta S_{\text{total}} \sim k_B N^2 - k_B N \log 2 \approx k_B N^2 > 0
\end{equation}

Therefore, there is no entropy paradox. The demon's operation increases total entropy, preserving the Second Law. The apparent paradox arose from ignoring categorical degrees of freedom and focusing only on spatial entropy. \qed
\end{proof}

\begin{remark}[Resolution of the Paradox]
\label{rem:paradox_resolution}
Corollary~\ref{cor:no_entropy_paradox} provides the complete resolution of Maxwell's demon paradox within the categorical framework. The resolution does not invoke information-theoretic arguments (Landauer's principle, erasure costs, measurement backaction) but rather recognises that entropy has two components: spatial entropy (counted by spatial microstates) and categorical entropy (counted by network edges). The demon's operation trades spatial entropy for categorical entropy, with the categorical increase dominating. This is fundamentally different from the information-theoretic resolution, which argues that the demon must pay an entropy cost to erase its memory. In the categorical framework, there is no demon, no memory, and no erasure—only categorical completion following network topology, which naturally increases entropy through network densification.
\end{remark}

\begin{figure*}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{figures/panel_arg6_dissolution_second_law.png}
\caption{\textbf{Argument 6: Dissolution of Second Law Violation—Categorical Entropy Increase Compensates.}
\textbf{(A)} Two entropy components with opposite trends. Spatial entropy $S_{\text{spatial}}$ (red line) decreases during apparent sorting as molecules become spatially segregated, following $S_{\text{spatial}} = -k_B \sum_i p_i^{\text{spatial}} \ln p_i^{\text{spatial}}$. Categorical entropy $S_{\text{categorical}}$ (green line) increases as the phase-lock network densifies, with $S_{\text{categorical}} = -k_B \sum_{\alpha} p_{\alpha}^{\text{cat}} \ln p_{\alpha}^{\text{cat}}$ where $\alpha$ indexes categorical states. Total entropy $S_{\text{total}} = S_{\text{spatial}} + S_{\text{categorical}}$ (dark teal dashed line) always increases, satisfying the second law. The gray dotted line at $S = 2.0$ marks the initial equilibrium value. The divergence of spatial and categorical components reveals the hidden entropy production.
\textbf{(B)} Network densification produces categorical entropy. The number of network edges increases from $\sim 100$ to $264$ over 50 sorting attempts, representing a gain of $+164$ edges (marked in red). Network density $\rho = 2|E|/(|V|(|V|-1))$ increases as molecules form more phase-lock connections. This densification corresponds to increased categorical entropy: $\Delta S_{\text{categorical}} = k_B \ln(\Omega_{\text{final}}/\Omega_{\text{initial}})$ where $\Omega$ is the number of accessible categorical states. The filled green area under the curve represents accumulated categorical entropy. The steep increase demonstrates that apparent sorting creates extensive network structure.
\textbf{(C)} Total entropy change distribution confirms $\Delta S_{\text{total}} > 0$. Histograms show the distribution of entropy changes across many trials. Spatial entropy changes (red) are predominantly negative ($\Delta S_{\text{spatial}} < 0$, left of dashed line at $\Delta S = 0$), confirming apparent sorting. Categorical entropy changes (green) are predominantly positive ($\Delta S_{\text{categorical}} > 0$, right of line). Crucially, total entropy changes (dark teal) are always positive, with the distribution centered at $\Delta S_{\text{total}} \approx +0.2$ (marked by green text ``$\Delta S > 0$''). The vertical dashed line at $\Delta S = 0$ separates second law violations (left, forbidden) from allowed processes (right). No trials violate the second law.
\textbf{(D)} Second law accounting shows net entropy increase. Bar chart quantifying entropy changes: spatial entropy decreases by $\Delta S_{\text{spatial}} = -0.3$ (red bar, apparent violation), categorical entropy increases by $\Delta S_{\text{categorical}} = +0.5$ (green bar, hidden compensation), yielding total entropy increase $\Delta S_{\text{total}} = +0.2 > 0$ (dark teal bar, second law satisfied). The numerical values demonstrate that categorical entropy production exceeds spatial entropy reduction by a factor of $\sim 1.7$, providing a comfortable margin. The second law is never violated; the demon's apparent sorting is compensated by hidden network entropy. This resolves the paradox: there is no thermodynamic violation because categorical completion increases total entropy.}
\label{fig:dissolution_second_law}
\end{figure*}
\end{proof}

\subsection{Entropy as Shortest Path}

We now provide an alternative characterisation of entropy in terms of path length in categorical state space. This geometric perspective reveals that entropy quantifies how "close" a system is to equilibrium in the categorical topology.

\begin{definition}[Oscillatory Termination Probability]
\label{def:termination_probability}
For a system in categorical state $C \in \catspace$, the \textbf{oscillatory termination probability} $\alpha(C)$ is the probability that the system's oscillatory dynamics reach equilibrium (terminate further categorical evolution) at state $C$. Equilibrium is characterised by the absence of accessible categorical transitions: all phase-lock pathways from $C$ lead to states that are already completed or equivalent to $C$ under the relevant equivalence relations. The termination probability quantifies the stability of state $C$: states with high $\alpha(C)$ are stable attractors in categorical space, while states with low $\alpha(C)$ are transient and likely to evolve further.
\end{definition}

\begin{remark}[Termination vs. Equilibrium]
\label{rem:termination_equilibrium}
Termination in categorical space corresponds to thermodynamic equilibrium in physical space. A system at equilibrium has completed all accessible categorical states and has no further categorical evolution available (all pathways lead to equivalent or already-completed states). This is analogous to a dynamical system reaching a fixed point or attractor. The termination probability $\alpha(C)$ is thus a measure of how "equilibrium-like" state $C$ is: high-entropy states (close to equilibrium) have high termination probability, while low-entropy states (far from equilibrium) have low termination probability.
\end{remark}

\begin{theorem}[Entropy as Path Length]
\label{thm:entropy_path}
Entropy is inversely related to the shortest path length to oscillatory termination in categorical state space. Specifically:
\begin{equation}
S(C) = -k_B \log \ell_{\text{term}}(C)
\label{eq:entropy_path}
\end{equation}
where $\ell_{\text{term}}(C)$ is the length of the shortest path from state $C$ to any termination state (equilibrium state) in $\catspace$, measured in the graph metric induced by phase-lock adjacency. Higher entropy corresponds to shorter paths to equilibrium: high-entropy states are "close" to equilibrium in categorical topology, while low-entropy states are "far" from equilibrium.
\end{theorem}

\begin{proof}
We establish the relationship between termination probability, path length, and entropy through a series of steps.

\textbf{Step 1: Termination probability and path length.}
The probability that a system starting at state $C$ terminates (reaches equilibrium) is inversely related to the distance from $C$ to the nearest equilibrium state. Intuitively, systems closer to equilibrium are more likely to reach equilibrium quickly, while systems far from equilibrium must traverse many categorical transitions before terminating. In the graph metric on $\catspace$ induced by phase-lock adjacency (where the distance between states is the length of the shortest path connecting them), the termination probability scales as:
\begin{equation}
\alpha(C) \propto \frac{1}{\ell_{\text{term}}(C)}
\end{equation}

This inverse relationship arises because each categorical transition has a finite probability of occurring (determined by phase-lock coupling strengths and thermal fluctuations), and the probability of completing a path of length $\ell$ is approximately $p^\ell$ for some transition probability $p < 1$. Therefore, longer paths have exponentially lower probability:
\begin{equation}
\alpha(C) \sim p^{\ell_{\text{term}}(C)} = \exp[\log(p) \cdot \ell_{\text{term}}(C)] \propto \frac{1}{\ell_{\text{term}}(C)}
\end{equation}
for $\log(p) \approx -1$ (transition probability $p \approx 1/e$).

\textbf{Step 2: Entropy and termination probability.}
Entropy is defined thermodynamically as $S = k_B \log \Omega$, where $\Omega$ is the number of accessible microstates. In categorical space, $\Omega$ is the number of categorical states accessible from the current state. States close to equilibrium have many accessible states (high $\Omega$, high $S$) because they are in the dense core of the categorical network. States far from equilibrium have few accessible states (low $\Omega$, low $S$) because they are on the periphery of the network.

The termination probability $\alpha(C)$ is related to the accessibility: states with high accessibility (many paths leading to them) have high termination probability. Therefore:
\begin{equation}
S(C) = k_B \log \Omega(C) \propto k_B \log \alpha(C)
\end{equation}

\textbf{Step 3: Combining the relationships.}
From Step 1, $\alpha(C) \propto 1/\ell_{\text{term}}(C)$. From Step 2, $S(C) \propto k_B \log \alpha(C)$. Combining:
\begin{equation}
S(C) = k_B \log \alpha(C) = k_B \log \left(\frac{1}{\ell_{\text{term}}(C)}\right) = -k_B \log \ell_{\text{term}}(C)
\end{equation}

This proves equation~\eqref{eq:entropy_path}. Higher entropy corresponds to shorter paths to termination: the system is "closer" to equilibrium in categorical space. Lower entropy corresponds to longer paths: the system is "farther" from equilibrium and must undergo more categorical transitions before reaching equilibrium. \qed
\end{proof}

\begin{corollary}[Entropy Increase as Path Optimization]
\label{cor:path_optimisation}
The Second Law of thermodynamics, which states that entropy increases monotonically in isolated systems, can be reinterpreted as a path optimization principle in categorical space. Specifically, the inequality:
\begin{equation}
\frac{dS}{dt} \geq 0
\end{equation}
is equivalent to:
\begin{equation}
\frac{d\ell_{\text{term}}}{dt} \leq 0
\end{equation}

Systems evolve toward shorter paths to termination. They optimize their route to equilibrium through categorical space, following phase-lock pathways that minimize the remaining distance to equilibrium. Entropy increase is thus a consequence of categorical path optimization, not a separate thermodynamic principle.
\end{corollary}

\begin{proof}
From Theorem~\ref{thm:entropy_path}, $S(C) = -k_B \log \ell_{\text{term}}(C)$. Differentiating with respect to time:
\begin{equation}
\frac{dS}{dt} = -k_B \frac{d}{dt}[\log \ell_{\text{term}}] = -k_B \frac{1}{\ell_{\text{term}}} \frac{d\ell_{\text{term}}}{dt}
\end{equation}

The Second Law states $dS/dt \geq 0$. Therefore:
\begin{equation}
-k_B \frac{1}{\ell_{\text{term}}} \frac{d\ell_{\text{term}}}{dt} \geq 0
\end{equation}

Since $k_B > 0$ and $\ell_{\text{term}} > 0$, this implies:
\begin{equation}
\frac{d\ell_{\text{term}}}{dt} \leq 0
\end{equation}

The path length to termination decreases monotonically. The system follows categorical pathways that bring it closer to equilibrium at each step, optimizing its trajectory through categorical space. This is analogous to gradient descent in optimization: the system follows the steepest descent direction in the categorical topology, minimizing the distance to equilibrium. \qed
\end{proof}

\begin{remark}[Geometric Interpretation of Second Law]
\label{rem:geometric_second_law}
Corollary~\ref{cor:path_optimisation} provides a geometric interpretation of the Second Law. Entropy increase is not a statistical tendency or a consequence of information loss but rather a geometric necessity: systems follow phase-lock pathways in categorical space, and these pathways are directed toward equilibrium (shorter paths). The categorical topology has a natural gradient structure, with equilibrium states as attractors. Systems evolve along this gradient, necessarily decreasing the distance to equilibrium and thereby increasing entropy. This geometric perspective unifies thermodynamics with dynamical systems theory and topology.
\end{remark}

\subsection{Why "Sorting" Increases Entropy}

We now apply the topological entropy framework to Maxwell's demon, proving that the demon's apparent "sorting" operation actually increases entropy through network densification. This provides the final resolution of the paradox.

\begin{theorem}[Sorting Increases Network Density]
\label{thm:sorting_density}
The operation attributed to Maxwell's Demon—categorical selection and pathway following—increases phase-lock network density and therefore increases entropy. The demon does not violate the Second Law because its operation is categorical completion, which naturally increases categorical entropy faster than it decreases spatial entropy.
\end{theorem}

\begin{proof}
We analyze the demon's operation as a sequence of categorical completions and show that each step increases network density.

\textbf{Step 1: Initial selection.}
The demon begins by "observing" a molecule, which in categorical terms is selecting a specific categorical state $C_1$ from the equivalence class $[C]_{\text{spatial}}$ of states that are indistinguishable by spatial configuration alone. From Theorem~\ref{thm:information_free}, this selection is information-free: it requires no external information input because it is determined by phase-lock network topology (which states are adjacent to already-completed states). The selection completes state $C_1$, making it an actual state rather than a potential state.

Completing $C_1$ makes phase-lock adjacent states accessible. From Theorem~\ref{thm:phase_lock_accessibility}, the accessible states are:
\begin{equation}
\accessible(C_1) = \{C_j \in \catspace : (C_1, C_j) \in E_{\text{PL}}\}
\end{equation}
where $E_{\text{PL}}$ is the set of phase-lock adjacency edges in categorical space. These adjacent states were not accessible before $C_1$ was completed (they were potential but not reachable). After completion, they become accessible, effectively adding edges to the reachable network.

\textbf{Step 2: Cascade propagation.}
From Theorem~\ref{thm:categorical_cascade}, completing state $C_1$ initiates a cascade of completions through the phase-lock network. Each accessible state $C_j \in \accessible(C_1)$ can now be completed, which in turn makes states in $\accessible(C_j)$ accessible, and so on. The cascade propagates through the network until it reaches a boundary (states with no further accessible states) or returns to already-completed states.

At each step $n$ of the cascade, the set of completed states grows:
\begin{equation}
\gamma(t_n) = \gamma(t_{n-1}) \cup \{C_n\}
\end{equation}
where $C_n$ is the newly completed state at step $n$. The effective phase-lock network—the subgraph of $\phaselockgraph$ induced by completed states—grows correspondingly.

\textbf{Step 3: Network densification.}
As more categorical states are completed, the effective phase-lock network densifies. The number of edges in the effective network at time $t$ is:
\begin{equation}
|E(\gamma(t))| = \text{number of edges between completed states}
\end{equation}

This edge count increases monotonically with time:
\begin{equation}
|E(\gamma(t_2))| > |E(\gamma(t_1))| \quad \text{for } t_2 > t_1
\end{equation}

The increase occurs because completing a new state $C_n$ adds all edges connecting $C_n$ to previously completed states. If $C_n$ has $d_n$ neighbors in the phase-lock network (its degree), then completing $C_n$ adds up to $d_n$ new edges to the effective network. Since typical phase-lock networks have mean degree $\langle d \rangle \sim 5$-$10$ (each molecule is phase-locked to several neighbors), each completion adds several edges.

Moreover, completed states cannot be uncompleted. From Axiom~\ref{axiom:categorical_irreversibility}, categorical completion is irreversible: once a state is completed, it remains completed. Therefore, edges added to the effective network are never removed. The network can only densify, never sparsify.

\textbf{Step 4: Entropy increase.}
From Proposition~\ref{prop:entropy_edge_density}, entropy is proportional to edge count:
\begin{equation}
S_{\phaselockgraph}(t) \propto k_B |E(\gamma(t))|
\end{equation}

Since $|E(\gamma(t))|$ increases monotonically, entropy increases monotonically:
\begin{equation}
\Delta S = S(t_2) - S(t_1) = k_B [|E(\gamma(t_2))| - |E(\gamma(t_1))|] = k_B \Delta |E| > 0
\end{equation}

The demon's operation—categorical selection and cascade propagation—increases entropy. The apparent "sorting" does not decrease entropy because it is not spatial rearrangement but categorical completion. The demon adds categorical structure (phase-lock relationships), which increases entropy faster than any spatial ordering decreases entropy.

Quantitatively, sorting $N$ molecules into two compartments decreases spatial entropy by $\Delta S_{\text{spatial}} \sim -k_B N \log 2$. But categorical completion adds $\Delta |E| \sim N \langle d \rangle$ edges (each of $N$ molecules contributes its degree $\langle d \rangle$ to the edge count), increasing entropy by $\Delta S_{\text{categorical}} \sim k_B N \langle d \rangle$. For typical phase-lock networks with $\langle d \rangle \sim 5$-$10$, the categorical increase dominates:
\begin{equation}
\Delta S_{\text{total}} = \Delta S_{\text{spatial}} + \Delta S_{\text{categorical}} \sim -k_B N \log 2 + k_B N \langle d \rangle \approx k_B N (\langle d \rangle - 0.7) > 0
\end{equation}

Therefore, the demon's operation increases total entropy, preserving the Second Law. \qed
\end{proof}

\begin{corollary}[Second Law Preserved]
\label{cor:second_law}
Maxwell's Demon, reinterpreted as categorical completion through phase-lock topology, does not violate the Second Law of Thermodynamics. The apparent paradox arose from three conceptual errors: misidentifying the demon's operation as sorting by kinetic energy rather than categorical completion, ignoring categorical degrees of freedom by focusing only on spatial configuration, and failing to account for categorical entropy in the total entropy balance. When categorical structure is properly accounted for, entropy increases monotonically even during the demon's operation.
\end{corollary}

\begin{proof}
The total entropy of the system has two components: spatial entropy $S_{\text{spatial}}$ (quantified by Boltzmann's formula $S_{\text{spatial}} = k_B \log \Omega_{\text{spatial}}$, where $\Omega_{\text{spatial}}$ is the number of spatial microstates) and categorical entropy $S_{\text{categorical}}$ (quantified by network entropy $S_{\text{categorical}} = k_B \log \Omega_{\text{PL}}$, where $\Omega_{\text{PL}}$ is the number of categorical states). The total entropy is:
\begin{equation}
S_{\text{total}} = S_{\text{spatial}} + S_{\text{categorical}}
\end{equation}

The time derivative is:
\begin{equation}
\frac{dS_{\text{total}}}{dt} = \frac{dS_{\text{spatial}}}{dt} + \frac{dS_{\text{categorical}}}{dt}
\end{equation}

During the demon's operation, spatial entropy may decrease ($dS_{\text{spatial}}/dt < 0$) due to the apparent ordering of molecules into hot and cold compartments. This is the source of the paradox: the demon appears to decrease entropy by creating order.

However, categorical entropy increases ($dS_{\text{categorical}}/dt > 0$) due to network densification (Theorem~\ref{thm:sorting_density}). The categorical increase dominates the spatial decrease because the number of phase-lock edges added by categorical completion ($\Delta |E| \sim N \langle d \rangle$) is much larger than the reduction in spatial microstates ($\Delta \Omega_{\text{spatial}} \sim 2^{-N}$, corresponding to sorting into two compartments).

Therefore:
\begin{equation}
\frac{dS_{\text{total}}}{dt} = \frac{dS_{\text{spatial}}}{dt} + \frac{dS_{\text{categorical}}}{dt} \geq 0
\end{equation}

The Second Law is preserved. The inequality is strict ($> 0$) during active categorical completion and becomes an equality ($= 0$) only at equilibrium when all accessible categorical states have been completed and no further network densification is possible.

The apparent paradox arose from ignoring the categorical entropy term. Traditional thermodynamic analyses focus exclusively on spatial entropy (or equivalently, kinetic energy distribution), treating entropy as $S = S_{\text{spatial}}$ only. This incomplete accounting makes it appear that the demon decreases entropy. When categorical entropy is included, the full entropy balance shows monotonic increase, and the paradox dissolves. \qed
\end{proof}

\begin{remark}[Comparison with Information-Theoretic Resolution]
\label{rem:comparison_information}
The categorical resolution of Maxwell's demon paradox differs fundamentally from the information-theoretic resolution (Landauer's principle and subsequent refinements). The information-theoretic resolution argues that the demon must pay an entropy cost to erase its memory after each measurement, and this cost exactly compensates for the entropy decrease in the gas. The categorical resolution argues that there is no entropy decrease in the first place: the demon's operation increases total entropy (spatial plus categorical) from the outset, so no compensating cost is needed. The information-theoretic resolution preserves the Second Law by adding a hidden entropy cost; the categorical resolution preserves the Second Law by recognizing a hidden entropy increase. Both resolutions are mathematically consistent, but they offer different physical interpretations. The categorical resolution has the advantage of not requiring a demon, memory, or erasure—it explains entropy increase through network topology alone, without invoking information theory.
\end{remark}

