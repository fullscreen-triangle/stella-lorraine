\section{Entropy from Triple Equivalence}

\subsection{The Physical Content of Mathematical Equivalence}

Section 1 established that bounded physical systems admit three mathematically equivalent descriptions: oscillatory, categorical, and partition. Each uses different mathematical objects (continuous functions, discrete sets, combinatorial operations) and different conceptual frameworks (dynamics, logic, arithmetic).

A fundamental question: does this mathematical equivalence extend to physical quantities? Specifically, does entropy—the central quantity of statistical mechanics and thermodynamics—have the same value when computed from each perspective?

We demonstrate that the answer is yes. Entropy emerges naturally from each perspective without additional statistical assumptions. The three derivations are independent, yet yield identical results. This convergence follows directly from the triple equivalence and the mandatory convergence principle (Section 1.5).

\subsection{The Pendulum as Prototype System}

We use the simple pendulum as our prototype bounded system. The pendulum satisfies all boundedness conditions:
\begin{itemize}
\item \textbf{Spatial:} Angular displacement $\theta \in [-\theta_{\max}, \theta_{\max}]$ where $\theta_{\max}$ is determined by energy
\item \textbf{Energetic:} Total energy $E = \frac{1}{2}mL^2\dot{\theta}^2 + mgL(1-\cos\theta) \leq E_{\max}$
\item \textbf{Temporal:} Period $T = 2\pi/\omega$ where $\omega = \sqrt{g/L}$ for small oscillations
\end{itemize}

Results derived for the pendulum generalize immediately to arbitrary bounded systems through the Poincaré recurrence theorem.

\subsection{Entropy from Oscillatory Mechanics}

\subsubsection{Phase Space Structure}

The oscillatory description characterizes the pendulum through continuous variables $(\theta, \dot{\theta})$ in phase space. Over one complete period $T$, the system traces a closed trajectory. The area enclosed by this trajectory is the accessible phase space volume $\Omega$.

For a harmonic oscillator with energy $E$:
\begin{equation}
\Omega = \oint p \, dq = 2\pi E/\omega
\end{equation}

For a pendulum with amplitude $A$ (maximum angular displacement):
\begin{equation}
E = \frac{1}{2}mL^2\omega^2 A^2 \implies \Omega = \pi mL^2\omega A^2
\end{equation}

\subsubsection{State Counting from Temporal Resolution}

To observe the pendulum with temporal resolution $\tau$, sample its state at discrete times $t_k = k\tau$ for $k \in \{0, 1, \ldots, n-1\}$ where $n = T/\tau$.

Each sample captures a distinguishable phase. Two samples separated by time $\tau$ correspond to different points on the phase space trajectory. The number of distinguishable states equals the number of samples: $n = T/\tau$.

Equivalently, through phase space volume: each distinguishable state occupies a cell of size $\Delta\Omega = \Omega/n$, giving:
\begin{equation}
n = \frac{\Omega}{\Delta\Omega}
\end{equation}

For quantum systems, the natural cell size is $\Delta\Omega = 2\pi\hbar$ (one quantum state). For classical systems, $\Delta\Omega$ is determined by measurement resolution.

\subsubsection{Oscillatory Entropy}

Define oscillatory entropy as the logarithm of distinguishable states:
\begin{equation}
S_{\text{osc}} = k_B \ln n
\end{equation}

This is the Boltzmann entropy formula. It counts configurations consistent with macroscopic constraints (energy $E$, period $T$).

For a quantum oscillator with $n = \Omega/(2\pi\hbar)$:
\begin{equation}
S_{\text{osc}} = k_B \ln\left(\frac{\Omega}{2\pi\hbar}\right) = k_B \ln\left(\frac{E}{\hbar\omega}\right)
\end{equation}

For $M$ independent oscillatory modes (e.g., $M$ coupled pendulums or $M$ degrees of freedom), each contributes independently:
\begin{equation}
S_{\text{osc}} = k_B \sum_{i=1}^{M} \ln n_i = k_B M \ln\langle n\rangle
\end{equation}
where $\langle n\rangle$ is the average quantum number per mode.

\subsubsection{Physical Interpretation}

Oscillatory entropy measures phase space accessibility. A pendulum with large amplitude (high energy) has large $\Omega$, hence large $n$, hence large entropy. A pendulum with small amplitude (low energy) has small $\Omega$, small $n$, small entropy.

Crucially, this entropy depends only on macroscopic observables (energy $E$, period $T$, resolution $\tau$), not on microscopic trajectory details. Two pendulums with the same $E$ and $T$ have identical oscillatory entropy regardless of initial conditions.

\subsection{Entropy from Categorical Structure}

\subsubsection{Category Construction}

The categorical description divides the pendulum's period into discrete temporal intervals. Define $n$ categories corresponding to time windows:
\begin{equation}
C_k = \left\{\text{states with } t \in \left[k\frac{T}{n}, (k+1)\frac{T}{n}\right)\right\}, \quad k \in \{0, 1, \ldots, n-1\}
\end{equation}

Each category is a subset of phase space containing all states accessible during the $k$-th interval. The pendulum trajectory induces a deterministic sequence:
\begin{equation}
C_0 \to C_1 \to C_2 \to \cdots \to C_{n-1} \to C_0
\end{equation}

Categories are mutually exclusive (no overlap) and exhaustive (every state belongs to exactly one category).

\subsubsection{Microstate Counting}

Within each category $C_k$, there may be multiple distinguishable microstates. Define the \textit{category depth} $d_k$ as the number of distinguishable microstates within $C_k$:
\begin{equation}
d_k = \frac{\text{Phase space volume of } C_k}{\text{Resolution cell size}}
\end{equation}

For uniform sampling, all categories have equal depth $d_k = d$. The total number of distinguishable microstates is:
\begin{equation}
\Omega_{\text{cat}} = \sum_{k=0}^{n-1} d_k = nd
\end{equation}

If we distinguish only temporal categories (not internal structure), then $d = 1$ and $\Omega_{\text{cat}} = n$.

\subsubsection{Categorical Entropy}

Define categorical entropy as the logarithm of total microstates:
\begin{equation}
S_{\text{cat}} = k_B \ln \Omega_{\text{cat}}
\end{equation}

For uniform depth $d = 1$ (temporal categories only):
\begin{equation}
S_{\text{cat}} = k_B \ln n
\end{equation}

For non-uniform depth:
\begin{equation}
S_{\text{cat}} = k_B \ln(nd) = k_B \ln n + k_B \ln d
\end{equation}

The first term counts temporal categories; the second counts internal structure.

\subsubsection{Information-Theoretic Formulation}

An equivalent formulation uses Shannon entropy. If the system can occupy any of $n$ categories with probabilities $p_k$, the entropy is:
\begin{equation}
S_{\text{cat}} = -k_B \sum_{k=0}^{n-1} p_k \ln p_k
\end{equation}

For uniform sampling (equal time in each category), $p_k = 1/n$, giving:
\begin{equation}
S_{\text{cat}} = -k_B \sum_{k=0}^{n-1} \frac{1}{n} \ln\frac{1}{n} = k_B \ln n
\end{equation}

This recovers the microstate counting result.

\subsubsection{Physical Interpretation}

Categorical entropy measures distinguishable configurations. Fine temporal resolution ($\tau$ small, $n$ large) yields many categories, hence large entropy. Coarse resolution ($\tau$ large, $n$ small) yields few categories, hence small entropy.

This entropy depends on observational resolution $\tau$. Two observers with different resolutions compute different categorical entropies for the same pendulum. However, observers using the same resolution compute identical entropy—this is the convergence property.

\subsection{Entropy from Partition Operations}

\subsubsection{Partition Construction}

The partition description constructs temporal structure through recursive subdivision. Begin with the full period $[0, T]$. Apply partition operation $\Pi_n$ dividing this into $n$ equal parts:
\begin{equation}
\Pi_n: [0, T] \to \bigcup_{k=0}^{n-1} \left[k\frac{T}{n}, (k+1)\frac{T}{n}\right]
\end{equation}

Each interval $[k\tau, (k+1)\tau]$ where $\tau = T/n$ is a \textit{partition cell}. The partition depth is $n$. The partition width is $\tau$.

\subsubsection{Combinatorial Structure}

Partition cells have additive structure. Any interval $[0, k\tau]$ is constructed by concatenating $k$ elementary cells:
\begin{equation}
[0, k\tau] = \bigcup_{j=0}^{k-1} [j\tau, (j+1)\tau]
\end{equation}

Using additive notation with $\tau_1 = \tau$:
\begin{equation}
t_k = k\tau_1 = \underbrace{\tau_1 + \tau_1 + \cdots + \tau_1}_{k \text{ times}}
\end{equation}

This additive structure is fundamental: partition cells can be combined (concatenated) and decomposed (subdivided) using arithmetic operations.

\subsubsection{Distinguishable Partition Counting}

To define partition entropy, count distinguishable ways to partition the period. Two partitions are distinguishable if they differ observably.

For a pendulum with $n$ distinguishable positions (one per time interval), there are $n$ distinguishable partitions:
\begin{equation}
\Omega_{\text{part}} = n
\end{equation}

Taking the logarithm:
\begin{equation}
S_{\text{part}} = k_B \ln n
\end{equation}

\subsubsection{Selectivity Formulation}

An alternative approach uses partition selectivity. Each partition cell "selects" a subset of phase space. If cell $k$ has selectivity $s_k$ (fraction of phase space it contains), the total configuration count is:
\begin{equation}
\Omega_{\text{part}} = \prod_{k=0}^{n-1} \frac{1}{s_k}
\end{equation}

For uniform partitioning, $s_k = 1/n$, giving:
\begin{equation}
\Omega_{\text{part}} = \left(\frac{1}{1/n}\right)^n = n^n
\end{equation}

But this overcounts. The correct counting recognizes that we enumerate distinguishable partition configurations, not independent cell selections. For $n$ cells with uniform selectivity, there are $n$ distinguishable configurations:
\begin{equation}
\Omega_{\text{part}} = n
\end{equation}

Therefore:
\begin{equation}
S_{\text{part}} = k_B \ln n
\end{equation}

\subsubsection{Physical Interpretation}

Partition entropy measures distinguishable subdivision schemes. A finely partitioned period ($n$ large) has more distinguishable configurations than a coarsely partitioned period ($n$ small).

The partition perspective emphasizes combinatorial structure: entropy arises from counting distinguishable arrangements, not from probabilistic assumptions. This is the most fundamental view—oscillatory and categorical entropies derive from partition entropy by identifying states and categories with partition cells.

\subsection{Entropy Equivalence Theorem}

Three independent derivations yield:
\begin{align}
S_{\text{osc}} &= k_B \ln n \quad \text{(phase space volume)} \\
S_{\text{cat}} &= k_B \ln n \quad \text{(microstate count)} \\
S_{\text{part}} &= k_B \ln n \quad \text{(configuration count)}
\end{align}

\begin{theorem}[Entropy Equivalence]
\label{thm:entropy_equivalence}
For any bounded system with temporal resolution $\tau = T/n$, entropy computed from oscillatory, categorical, and partition descriptions is identical:
\begin{equation}
S_{\text{osc}} = S_{\text{cat}} = S_{\text{part}} = k_B \ln n
\end{equation}
\end{theorem}

\begin{proof}
By the triple equivalence (Theorem 1.1), the three descriptions are related by bijections:
\begin{equation}
\text{Oscillatory state } t_k \leftrightarrow \text{Category } C_k \leftrightarrow \text{Partition cell } [k\tau, (k+1)\tau]
\end{equation}

Bijections preserve cardinality. Therefore:
\begin{equation}
n_{\text{osc}} = n_{\text{cat}} = n_{\text{part}} = n
\end{equation}

Entropy is the logarithm of state count (Boltzmann formula):
\begin{equation}
S = k_B \ln(\text{number of states})
\end{equation}

Therefore:
\begin{align}
S_{\text{osc}} &= k_B \ln n_{\text{osc}} = k_B \ln n \\
S_{\text{cat}} &= k_B \ln n_{\text{cat}} = k_B \ln n \\
S_{\text{part}} &= k_B \ln n_{\text{part}} = k_B \ln n
\end{align}

All three entropies are equal.
\end{proof}

\subsection{Extension to Multiple Degrees of Freedom}

\subsubsection{Independent Oscillators}

Consider $M$ independent pendulums, each with a period $T_i$ and a resolution $\tau_i$, giving a state count $n_i = T_i/\tau_i$. The total phase space is the Cartesian product:
\begin{equation}
\Omega_{\text{total}} = \prod_{i=1}^{M} n_i
\end{equation}

Taking the logarithm:
\begin{equation}
S_{\text{total}} = k_B \ln\left(\prod_{i=1}^{M} n_i\right) = k_B \sum_{i=1}^{M} \ln n_i
\end{equation}

For identical oscillators ($n_i = n$):
\begin{equation}
S_{\text{total}} = k_B M \ln n
\end{equation}

\subsubsection{Indistinguishable Particles}

For $N$ indistinguishable oscillators (identical gas molecules), divide by $N!$ to avoid overcounting:
\begin{equation}
\Omega_{\text{indist}} = \frac{n^N}{N!}
\end{equation}

Using Stirling's approximation $\ln N! \approx N\ln N - N$:
\begin{equation}
S_{\text{indist}} = k_B \ln\left(\frac{n^N}{N!}\right) = k_B N\ln\left(\frac{en}{N}\right)
\end{equation}

For $n \gg N$ (dilute limit):
\begin{equation}
S_{\text{indist}} \approx k_B N \ln n
\end{equation}

This is the Sackur-Tetrode formula for ideal gas entropy.

\subsubsection{Coupled Oscillators}

For coupled oscillators (atoms in a solid), normal modes become the relevant degrees of freedom. Each normal mode contributes $k_B \ln n_i$ where $n_i$ is the excitation level:
\begin{equation}
S_{\text{coupled}} = k_B \sum_{i=1}^{M} \ln n_i
\end{equation}

This is the Debye model (phonons) or Planck distribution (photons).

\subsection{Connection to Thermodynamic Entropy}

\subsubsection{Thermodynamic Definition}

Classical thermodynamics defines entropy through reversible heat transfer:
\begin{equation}
dS_{\text{thermo}} = \frac{\delta Q_{\text{rev}}}{T}
\end{equation}

For a system with energy $E$ and temperature $T$:
\begin{equation}
S_{\text{thermo}} = \int \frac{dE}{T}
\end{equation}

\subsubsection{Statistical Temperature}

From oscillatory mechanics, energy relates to quantum number:
\begin{equation}
E = \hbar\omega n \implies n = \frac{E}{\hbar\omega}
\end{equation}

Statistical entropy:
\begin{equation}
S_{\text{stat}} = k_B \ln n = k_B \ln\left(\frac{E}{\hbar\omega}\right)
\end{equation}

Taking the derivative:
\begin{equation}
\frac{\partial S_{\text{stat}}}{\partial E} = \frac{k_B}{E}
\end{equation}

By thermodynamic definition, $\partial S/\partial E = 1/T$. Therefore:
\begin{equation}
\frac{1}{T} = \frac{k_B}{E} \implies T = \frac{E}{k_B}
\end{equation}

This defines temperature statistically: energy per degree of freedom (in units of $k_B$).

For $M$ degrees of freedom with total energy $U = ME$:
\begin{equation}
T = \frac{U}{Mk_B}
\end{equation}

Substituting back:
\begin{equation}
S_{\text{stat}} = k_B M \ln\left(\frac{T k_B}{\hbar\omega}\right) = Mk_B \ln T + \text{const}
\end{equation}

This matches thermodynamic entropy $S_{\text{thermo}} = \int (Mk_B/T) dT = Mk_B \ln T + \text{const}$.

Statistical and thermodynamic entropy are identical up to conventional additive constants.

\subsection{Resolution Dependence and Observer Independence}

\subsubsection{Apparent Resolution Dependence}

Two observers with different temporal resolutions $\tau_1$ and $\tau_2$ compute different state counts:
\begin{equation}
n_1 = \frac{T}{\tau_1}, \quad n_2 = \frac{T}{\tau_2}
\end{equation}

Therefore different entropies:
\begin{equation}
S_1 = k_B \ln n_1, \quad S_2 = k_B \ln n_2
\end{equation}

The difference:
\begin{equation}
S_2 - S_1 = k_B \ln\left(\frac{\tau_1}{\tau_2}\right)
\end{equation}

\subsubsection{Entropy Differences Are Observer-Independent}

If both observers measure the same system in two states (A and B):
\begin{align}
\Delta S_1 &= S_1^B - S_1^A = k_B \ln\left(\frac{n_1^B}{n_1^A}\right) \\
\Delta S_2 &= S_2^B - S_2^A = k_B \ln\left(\frac{n_2^B}{n_2^A}\right)
\end{align}

If both use consistent resolution:
\begin{equation}
\frac{n_1^B}{n_1^A} = \frac{T^B/\tau_1}{T^A/\tau_1} = \frac{T^B}{T^A} = \frac{T^B/\tau_2}{T^A/\tau_2} = \frac{n_2^B}{n_2^A}
\end{equation}

Therefore:
\begin{equation}
\Delta S_1 = \Delta S_2
\end{equation}

Entropy differences are observer-independent, confirming mandatory convergence.

\subsubsection{Physical Significance}

Resolution-dependence of absolute entropy reflects that entropy measures information content relative to measurement resolution. Resolution dependence of absolute entropy reflects that entropy measures information content relative to measurement resolution.

Physically relevant quantities are entropy differences (determining heat flow, work extraction), which are observer-independent as required by the objectivity criterion (Section 1.4).

\subsection{Summary}

We have derived entropy from three independent perspectives:

\textbf{Oscillatory:} Phase space volume $\Omega$ → state count $n = \Omega/(2\pi\hbar)$ → entropy $S = k_B \ln n$

\textbf{Categorical:} Temporal categories $C_k$ → microstate count $n$ → entropy $S = k_B \ln n$

\textbf{Partition:} Partition cells → configuration count $n$ → entropy $S = k_B \ln n$

All three yield $S = k_B \ln n$. Convergence is exact, not approximate—a mathematical necessity from the triple equivalence.

\begin{corollary}[Entropy as Intrinsic Property]
Any complete description of a bounded system yields identical entropy. Entropy is an intrinsic property of the system, not the description.
\end{corollary}

This establishes that entropy—the central quantity of thermodynamics—emerges identically from oscillatory, categorical, and partition perspectives. Subsequent sections extend this result to other thermodynamic quantities (temperature, pressure, free energy), demonstrating that thermodynamic structure follows from the triple equivalence.
