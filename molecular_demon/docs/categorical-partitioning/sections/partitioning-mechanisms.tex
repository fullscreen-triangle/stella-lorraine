\section{Entropy from Partition Mechanics}
\label{sec:partition}

We derive entropy from first principles of partition operations, making no reference to oscillatory dynamics or categorical structure. The derivation rests solely on the combinatorics of dividing systems into distinguishable parts.

\subsection{Axioms of Partition Operations}

\begin{axiom}[Partition Existence]
\label{axiom:partition_exist}
Any system $X$ with structure can be partitioned into subsystems. A \emph{partition} of $X$ is a collection $\{X_1, X_2, \ldots, X_n\}$ such that:
\begin{enumerate}[(i)]
    \item $X_i \cap X_j = \emptyset$ for $i \neq j$ (disjointness)
    \item $\bigcup_{i=1}^{n} X_i = X$ (exhaustiveness)
    \item Each $X_i$ is non-empty (non-triviality)
\end{enumerate}
\end{axiom}

\begin{axiom}[Branching Factor]
\label{axiom:branching}
Each partition operation divides a system into $n$ subsystems, where $n \geq 2$ is the \emph{branching factor}. The branching factor is determined by the structure of the system being partitioned.
\end{axiom}

\begin{axiom}[Recursive Partitionability]
\label{axiom:recursive_part}
Each subsystem $X_i$ produced by a partition is itself partitionable, admitting the same partition structure as the parent system. Partitioning can be applied recursively to arbitrary depth.
\end{axiom}

\begin{definition}[Partition Tree]
\label{def:partition_tree}
A \emph{partition tree} of depth $k$ is the structure produced by applying $k$ successive partition operations to an initial system $X^{(0)}$. At each level $j \in \{1, 2, \ldots, k\}$, each node from level $j-1$ is partitioned into $n$ child nodes.
\end{definition}

\subsection{Combinatorics of Partition Trees}

\begin{theorem}[Number of Partition Paths]
\label{thm:partition_paths}
For a partition tree of depth $k$ with branching factor $n$, the number of distinct paths from root to leaf is:
\begin{equation}
    P(k, n) = n^k
\end{equation}
\end{theorem}

\begin{proof}
At each of the $k$ levels of the partition tree, there are $n$ choices for which branch to follow. The total number of distinct paths is the product of choices at each level:
\begin{equation}
    P(k, n) = \underbrace{n \times n \times \cdots \times n}_{k \text{ times}} = n^k
\end{equation}
\end{proof}

\begin{theorem}[Number of Leaf Nodes]
\label{thm:leaf_nodes}
A partition tree of depth $k$ with branching factor $n$ has exactly $n^k$ leaf nodes.
\end{theorem}

\begin{proof}
At level 0 (the root), there is 1 node. At level 1, each node branches into $n$ children, giving $n$ nodes. At level 2, each of the $n$ nodes branches into $n$ children, giving $n^2$ nodes. By induction, at level $k$, there are $n^k$ nodes, all of which are leaves.
\end{proof}

\subsection{Partition Entropy from Branching Structure}

\begin{definition}[Partition Entropy]
\label{def:partition_entropy}
The \emph{partition entropy} of a partition tree measures the uncertainty about which leaf node (terminal partition element) is occupied when traversing the tree from root to leaf.
\end{definition}

\begin{theorem}[Partition Entropy per Level]
\label{thm:entropy_per_level}
Each partition operation (each level of the tree) contributes entropy:
\begin{equation}
    \Delta S_{\text{level}} = \kB \ln n
\end{equation}
where $n$ is the branching factor.
\end{theorem}

\begin{proof}
At each partition, a single parent system is divided into $n$ equiprobable child systems. If all branches are equally likely, the probability of each child is $p_i = 1/n$. The Shannon entropy of this uniform distribution is:
\begin{equation}
    H = -\sum_{i=1}^{n} \frac{1}{n} \ln \frac{1}{n} = \ln n
\end{equation}
Converting to thermodynamic entropy:
\begin{equation}
    \Delta S_{\text{level}} = \kB H = \kB \ln n
\end{equation}
\end{proof}

\begin{theorem}[Total Partition Entropy]
\label{thm:partition_entropy}
For a partition tree of depth $M$ with branching factor $n$, the total entropy is:
\begin{equation}
    \boxed{\Spart = \kB M \ln n}
\end{equation}
\end{theorem}

\begin{proof}
The total entropy is the sum of entropy contributions from each level. With $M$ levels, each contributing $\kB \ln n$:
\begin{equation}
    \Spart = \sum_{j=1}^{M} \Delta S_{\text{level}} = \sum_{j=1}^{M} \kB \ln n = \kB M \ln n
\end{equation}

Alternatively, using the Boltzmann relation directly: the number of distinguishable leaf nodes is $n^M$ (Theorem~\ref{thm:leaf_nodes}), so:
\begin{equation}
    \Spart = \kB \ln(n^M) = \kB M \ln n
\end{equation}
\end{proof}

\begin{remark}[Physical Interpretation]
The entropy $\Spart = \kB M \ln n$ has the following interpretation:
\begin{itemize}
    \item $M$ counts the number of partition levels (depth of recursive partitioning)
    \item $n$ counts the branching factor (number of parts per partition)
    \item $\ln n$ is the information generated per partition operation
    \item $\kB$ converts to thermodynamic units (J/K)
\end{itemize}
\end{remark}

\subsection{Dimensional Interpretation of Partition Depth}

\begin{theorem}[Partition Depth as Dimensionality]
\label{thm:depth_dimension}
For systems embedded in $d$-dimensional space, the natural partition branching factor is $n = d+1$ (dividing space into simplicial regions) or $n = 2^d$ (dividing space into hyperoctants). In three-dimensional space:
\begin{itemize}
    \item Simplicial partition: $n = 4$ (tetrahedral)
    \item Octant partition: $n = 8$ (cubic)
    \item Binary per dimension: $n = 2$ with $M = 3k$ levels
\end{itemize}
\end{theorem}

For the special case of tri-dimensional categorical structure with ternary branching:
\begin{equation}
    \Spart = \kB \cdot 3k \cdot \ln 3 = 3\kB k \ln 3
\end{equation}
where $k$ is the recursion depth.

\subsection{Sequential Partition and History Dependence}

\begin{definition}[Partition History]
\label{def:partition_history}
The \emph{partition history} $H_k$ of a system at depth $k$ is the sequence of partition choices made to reach the current state:
\begin{equation}
    H_k = (h_1, h_2, \ldots, h_k)
\end{equation}
where $h_j \in \{1, 2, \ldots, n\}$ specifies which branch was taken at level $j$.
\end{definition}

\begin{theorem}[History Encodes Entropy]
\label{thm:history_entropy}
The partition history $H_k$ encodes exactly $\Spart = \kB k \ln n$ bits of information. Systems with identical current states but different histories are entropically distinguishable.
\end{theorem}

\begin{proof}
The partition history is a sequence of $k$ symbols, each drawn from an alphabet of size $n$. The number of distinct histories is $n^k$. If all histories are equiprobable, the information content is:
\begin{equation}
    I = \ln(n^k) = k \ln n \text{ nats}
\end{equation}
Converting to thermodynamic entropy:
\begin{equation}
    \Spart = \kB I = \kB k \ln n
\end{equation}

The entropy of a system is determined by its full partition history, not merely its current configuration. Two systems in identical configurations but with different partition histories have different entropies.
\end{proof}

\subsection{Independence from Oscillatory and Categorical Concepts}

The derivation of $\Spart = \kB M \ln n$ relies solely on:
\begin{enumerate}
    \item Partition existence (Axiom~\ref{axiom:partition_exist})
    \item Constant branching factor (Axiom~\ref{axiom:branching})
    \item Recursive partitionability (Axiom~\ref{axiom:recursive_part})
    \item Boltzmann-Shannon entropy relation $S = \kB \ln W$
\end{enumerate}

No reference has been made to oscillatory dynamics, phase space structure, or categorical distinction. The entropy arises purely from the combinatorics of partition trees.

